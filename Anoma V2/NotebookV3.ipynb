{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.64      0.68        22\n",
      "           1       0.72      0.81      0.76        26\n",
      "\n",
      "    accuracy                           0.73        48\n",
      "   macro avg       0.73      0.72      0.72        48\n",
      "weighted avg       0.73      0.73      0.73        48\n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.62      0.70        24\n",
      "           1       0.69      0.83      0.75        24\n",
      "\n",
      "    accuracy                           0.73        48\n",
      "   macro avg       0.74      0.73      0.73        48\n",
      "weighted avg       0.74      0.73      0.73        48\n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.61      0.65        23\n",
      "           1       0.68      0.76      0.72        25\n",
      "\n",
      "    accuracy                           0.69        48\n",
      "   macro avg       0.69      0.68      0.68        48\n",
      "weighted avg       0.69      0.69      0.69        48\n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best parameters: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.76      0.78        25\n",
      "           1       0.75      0.78      0.77        23\n",
      "\n",
      "    accuracy                           0.77        48\n",
      "   macro avg       0.77      0.77      0.77        48\n",
      "weighted avg       0.77      0.77      0.77        48\n",
      "\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.69      0.73        26\n",
      "           1       0.68      0.77      0.72        22\n",
      "\n",
      "    accuracy                           0.73        48\n",
      "   macro avg       0.73      0.73      0.73        48\n",
      "weighted avg       0.74      0.73      0.73        48\n",
      "\n",
      "Best model accuracy: 0.7708333333333334\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Directory paths\n",
    "base_path = 'PhishingVoiceDataset'\n",
    "phishing_path = os.path.join(base_path, 'Phishing')\n",
    "non_phishing_path = os.path.join(base_path, 'NonPhishing')\n",
    "\n",
    "# Function to extract features (MFCCs) from an audio file\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_path)\n",
    "        return None \n",
    "    return mfccs_processed\n",
    "\n",
    "# Data augmentation function (time stretching and pitch shifting)\n",
    "def augment_audio(audio, sample_rate):\n",
    "    audio_time_stretch = librosa.effects.time_stretch(audio, rate=0.9)  # Time stretch\n",
    "    audio_pitch_shift = librosa.effects.pitch_shift(audio, sr=sample_rate, n_steps=2)  # Pitch shift\n",
    "    return audio_time_stretch, audio_pitch_shift\n",
    "\n",
    "# Parse files, extract features, and apply data augmentation\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for folder, label in [(phishing_path, 1), (non_phishing_path, 0)]:\n",
    "    for file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file)\n",
    "        if file_path.endswith('.mp3'):\n",
    "            audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "            mfccs = extract_features(file_path)\n",
    "            if mfccs is not None:\n",
    "                features.append(mfccs)\n",
    "                labels.append(label)\n",
    "                \n",
    "                # Apply Data Augmentation\n",
    "                augmented_time_stretch, augmented_pitch_shift = augment_audio(audio, sample_rate)\n",
    "                mfccs_aug_time = np.mean(librosa.feature.mfcc(y=augmented_time_stretch, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "                mfccs_aug_pitch = np.mean(librosa.feature.mfcc(y=augmented_pitch_shift, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "                \n",
    "                # Append augmented data\n",
    "                features.append(mfccs_aug_time)\n",
    "                labels.append(label)\n",
    "                features.append(mfccs_aug_pitch)\n",
    "                labels.append(label)\n",
    "\n",
    "# Convert into Numpy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Model Selection: Gradient Boosting Classifier with Grid Search for Hyperparameter Optimization\n",
    "model = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.05],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Cross-validation setup using KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# Train and evaluate the model using KFold Cross-Validation\n",
    "best_model = None\n",
    "best_score = 0\n",
    "for train_idx, test_idx in kfold.split(features_scaled):\n",
    "    X_train, X_test = features_scaled[train_idx], features_scaled[test_idx]\n",
    "    y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "    \n",
    "    # Fit the model using GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    score = grid_search.score(X_test, y_test)\n",
    "    \n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    # Save the best performing model\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "# Save the best model to disk\n",
    "joblib.dump(best_model, 'phishing_detection_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "print(f\"Best model accuracy: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: n5.mp3 is predicted as Non-Phishing with probability 0.66\n",
      "File: p1.mp3 is predicted as Phishing with probability 0.99\n",
      "File: n4.mp3 is predicted as Non-Phishing with probability 0.60\n",
      "File: n3.mp3 is predicted as Non-Phishing with probability 0.96\n",
      "File: p4.mp3 is predicted as Non-Phishing with probability 1.00\n",
      "File: p3.mp3 is predicted as Phishing with probability 1.00\n",
      "File: p5.mp3 is predicted as Phishing with probability 0.98\n",
      "File: p2.mp3 is predicted as Phishing with probability 0.92\n",
      "File: n2.mp3 is predicted as Non-Phishing with probability 0.98\n",
      "Bulk prediction completed. Results saved to phishing_detection_results.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Load the saved model and scaler\n",
    "loaded_model = joblib.load('phishing_detection_model.pkl')\n",
    "loaded_scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Function to extract features (MFCCs) from an audio file\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfccs_processed = np.mean(mfccs.T, axis=0)\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_path)\n",
    "        return None \n",
    "    return mfccs_processed\n",
    "\n",
    "# Function to predict on a new file\n",
    "def predict_phishing(file_path, model, scaler):\n",
    "    # Extract MFCCs from the audio file\n",
    "    features = extract_features(file_path)\n",
    "    \n",
    "    # Check if feature extraction was successful\n",
    "    if features is not None:\n",
    "        # Scale the features using the loaded scaler\n",
    "        features_scaled = scaler.transform([features])\n",
    "        \n",
    "        # Make the prediction (0 = Non-Phishing, 1 = Phishing)\n",
    "        prediction = model.predict(features_scaled)\n",
    "        proba = model.predict_proba(features_scaled)\n",
    "        \n",
    "        return prediction[0], proba\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Directory paths for test files\n",
    "test_folder_path = 'mnt'\n",
    "\n",
    "# Iterate over all files in the test folder and predict\n",
    "results = []\n",
    "\n",
    "for root, dirs, files in os.walk(test_folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.mp3'):  # Ensure only MP3 files are processed\n",
    "            file_path = os.path.join(root, file)\n",
    "            prediction, proba = predict_phishing(file_path, loaded_model, loaded_scaler)\n",
    "            \n",
    "            if prediction is not None:\n",
    "                if prediction == 1:\n",
    "                    result = f\"File: {file} is predicted as Phishing with probability {proba[0][1]:.2f}\"\n",
    "                else:\n",
    "                    result = f\"File: {file} is predicted as Non-Phishing with probability {proba[0][0]:.2f}\"\n",
    "                \n",
    "                print(result)  # Print each result to the console\n",
    "                results.append(result)\n",
    "            else:\n",
    "                print(f\"Failed to extract features from {file}\")\n",
    "\n",
    "# Save the results to a file\n",
    "with open('phishing_detection_results.txt', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(result + '\\n')\n",
    "\n",
    "print(\"Bulk prediction completed. Results saved to phishing_detection_results.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
